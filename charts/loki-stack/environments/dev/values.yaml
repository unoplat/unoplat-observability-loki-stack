test_pod:
  enabled: true
  image: bats/bats:1.8.2
  pullPolicy: IfNotPresent

loki:
  enabled: true
  isDefault: true
  url: http://{{(include "loki.serviceName" .)}}:{{ .Values.loki.service.port }}
  readinessProbe:
    httpGet:
      path: /ready
      port: http-metrics
    initialDelaySeconds: 45
  livenessProbe:
    httpGet:
      path: /ready
      port: http-metrics
    initialDelaySeconds: 45
  datasource:
    jsonData: "{}"
    uid: ""
  replicas: 3
  persistence:
    enabled: true
    accessModes:
      - ReadWriteOnce
    size: 5Gi
    labels: {}
    annotations: {}
  resources:
    limits:
      cpu: "2"
      memory: "3Gi"
    requests:
      cpu: "1"
      memory: "2Gi"
  affinity:
  podAntiAffinity:
    preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        podAffinityTerm:
          labelSelector:
            matchExpressions:
              - key: app
                operator: In
                values:
                  - loki
          topologyKey: "kubernetes.io/hostname"
  serviceMonitor:
    enabled: true
    interval: ""
    additionalLabels: {}
    annotations: {}
    # scrapeTimeout: 10s
    # path: /metrics
    scheme: null
    tlsConfig: {}
    prometheusRule:
      enabled: true
      additionalLabels: {}
      #  namespace:
      rules:
        #Some examples from https://awesome-prometheus-alerts.grep.to/rules.html#loki
        - alert: LokiProcessTooManyRestarts
          expr: changes(process_start_time_seconds{job=~"loki"}[15m]) > 2
          for: 0m
          labels:
            severity: warning
          annotations:
            summary: Loki process too many restarts (instance {{ $labels.instance }})
            description: "A loki process had too many restarts (target {{ $labels.instance }})\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
        - alert: LokiRequestErrors
          expr: 100 * sum(rate(loki_request_duration_seconds_count{status_code=~"5.."}[1m])) by (namespace, job, route) / sum(rate(loki_request_duration_seconds_count[1m])) by (namespace, job, route) > 10
          for: 15m
          labels:
            severity: critical
          annotations:
            summary: Loki request errors (instance {{ $labels.instance }})
            description: "The {{ $labels.job }} and {{ $labels.route }} are experiencing errors\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
        - alert: LokiRequestPanic
          expr: sum(increase(loki_panic_total[10m])) by (namespace, job) > 0
          for: 5m
          labels:
            severity: critical
          annotations:
            summary: Loki request panic (instance {{ $labels.instance }})
            description: "The {{ $labels.job }} is experiencing {{ printf \"%.2f\" $value }}% increase of panics\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
        - alert: LokiRequestLatency
          expr: (histogram_quantile(0.99, sum(rate(loki_request_duration_seconds_bucket{route!~"(?i).*tail.*"}[5m])) by (le)))  > 1
          for: 5m
          labels:
            severity: critical
          annotations:
            summary: Loki request latency (instance {{ $labels.instance }})
            description: "The {{ $labels.job }} {{ $labels.route }} is experiencing {{ printf \"%.2f\" $value }}s 99th percentile latency\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

promtail:
  resources:
    limits:
      cpu: 1000m
      memory: 512Mi
    requests:
      cpu: 500m
      memory: 128Mi
  enabled: true
  config:
    logLevel: info
    serverPort: 3101
    clients:
      - url: http://{{ .Release.Name }}:3100/loki/api/v1/push
  serviceMonitor:
    # -- If enabled, ServiceMonitor resources for Prometheus Operator are created
    enabled:
      true
      # -- Alternative namespace for ServiceMonitor resources
    namespace:
      null
      # -- Namespace selector for ServiceMonitor resources
    namespaceSelector:
      {}
      # -- ServiceMonitor annotations
    annotations:
      {}
      # -- Additional ServiceMonitor labels
    labels:
      {}
      # -- ServiceMonitor scrape interval
    interval:
      null
      # -- ServiceMonitor scrape timeout in Go duration format (e.g. 15s)
    scrapeTimeout:
      null
      # -- ServiceMonitor relabel configs to apply to samples before scraping
      # https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/api.md#relabelconfig
      # (defines `relabel_configs`)
    relabelings:
      []
      # -- ServiceMonitor relabel configs to apply to samples as the last
      # step before ingestion
      # https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/api.md#relabelconfig
      # (defines `metric_relabel_configs`)
    metricRelabelings:
      []
      # --ServiceMonitor will add labels from the service to the Prometheus metric
      # https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#servicemonitorspec
    targetLabels:
      []
      # -- ServiceMonitor will use http by default, but you can pick https as well
    scheme:
      http
      # -- ServiceMonitor will use these tlsConfig settings to make the health check requests
    tlsConfig:
      null
      # -- Prometheus rules will be deployed for alerting purposes
    prometheusRule:
      enabled: true
      additionalLabels: {}
      # namespace:
      rules:
        - alert: PromtailRequestErrors
          expr: 100 * sum(rate(promtail_request_duration_seconds_count{status_code=~"5..|failed"}[1m])) by (namespace, job, route, instance) / sum(rate(promtail_request_duration_seconds_count[1m])) by (namespace, job, route, instance) > 10
          for: 5m
          labels:
            severity: critical
          annotations:
            description: |
              The {{ $labels.job }} {{ $labels.route }} is experiencing
              {{ printf \"%.2f\" $value }} errors.
              VALUE = {{ $value }}
              LABELS = {{ $labels }}
            summary: Promtail request errors (instance {{ $labels.instance }})
        - alert: PromtailRequestLatency
          expr: histogram_quantile(0.99, sum(rate(promtail_request_duration_seconds_bucket[5m])) by (le)) > 1
          for: 5m
          labels:
            severity: critical
          annotations:
            summary: Promtail request latency (instance {{ $labels.instance }})
            description: |
              The {{ $labels.job }} {{ $labels.route }} is experiencing
              {{ printf \"%.2f\" $value }}s 99th percentile latency.
              VALUE = {{ $value }}
              LABELS = {{ $labels }}

fluent-bit:
  enabled: false

grafana:
  enabled: false
  sidecar:
    datasources:
      label: ""
      labelValue: ""
      enabled: true
      maxLines: 1000
  image:
    tag: 8.3.5

prometheus:
  enabled: false
  isDefault: false
  url: http://{{ include "prometheus.fullname" .}}:{{ .Values.prometheus.server.service.servicePort }}{{ .Values.prometheus.server.prefixURL }}
  datasource:
    jsonData: "{}"

filebeat:
  enabled: false
  filebeatConfig:
    filebeat.yml: |
      # logging.level: debug
      filebeat.inputs:
      - type: container
        paths:
          - /var/log/containers/*.log
        processors:
        - add_kubernetes_metadata:
            host: ${NODE_NAME}
            matchers:
            - logs_path:
                logs_path: "/var/log/containers/"
      output.logstash:
        hosts: ["logstash-loki:5044"]

logstash:
  enabled: false
  image: grafana/logstash-output-loki
  imageTag: 1.0.1
  filters:
    main: |-
      filter {
        if [kubernetes] {
          mutate {
            add_field => {
              "container_name" => "%{[kubernetes][container][name]}"
              "namespace" => "%{[kubernetes][namespace]}"
              "pod" => "%{[kubernetes][pod][name]}"
            }
            replace => { "host" => "%{[kubernetes][node][name]}"}
          }
        }
        mutate {
          remove_field => ["tags"]
        }
      }
  outputs:
    main: |-
      output {
        loki {
          url => "http://loki:3100/loki/api/v1/push"
          #username => "test"
          #password => "test"
        }
        # stdout { codec => rubydebug }
      }

# proxy is currently only used by loki test pod
# Note: If http_proxy/https_proxy are set, then no_proxy should include the
# loki service name, so that tests are able to communicate with the loki
# service.
proxy:
  http_proxy: ""
  https_proxy: ""
  no_proxy: ""
